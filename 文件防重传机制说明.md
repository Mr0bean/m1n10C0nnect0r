# 文件防重传（去重）机制说明

## 概述

系统实现了基于内容哈希（SHA-256）的文件去重机制，但目前该机制并未在实际上传流程中被完全激活使用。

## 核心组件

### 1. 内容哈希计算

**位置**: `minio-file-manager/backend/app/services/document_pipeline_service.py` 第155行

```python
# 以文件二进制计算内容哈希，作为幂等与去重的基础标识
content_hash = hashlib.sha256(file_content).hexdigest()
```

**特点**:
- 使用 SHA-256 算法对文件的完整二进制内容进行哈希
- 相同内容的文件会产生相同的哈希值
- 哈希值存储在 PostgreSQL 的 metadata 字段中

### 2. 重复检测函数

**位置**: `minio-file-manager/backend/app/services/postgresql_service.py` 第320-340行

```python
async def check_duplicate_by_content_hash(self, content_hash: str) -> Optional[str]:
    """通过内容哈希检查是否存在重复"""
    pool = await self.get_pool()
    
    try:
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id FROM public.newsletters 
                WHERE metadata->>'content_hash' = $1
                """,
                content_hash
            )
            
            if row:
                return row['id']  # 返回已存在记录的ID
            return None
```

**功能**:
- 通过查询 PostgreSQL 中的 metadata JSON 字段检查是否存在相同哈希值
- 如果找到匹配，返回已存在记录的 ID
- 如果没有找到，返回 None

### 3. 哈希值存储

**位置**: `minio-file-manager/backend/app/services/article_processing_service.py` 第296-301行

```python
metadata={
    'content_hash': content_hash,  # 存储文件内容哈希
    'file_format': extracted['format'],
    'word_count': extracted['word_count'],
    **extracted['metadata']
}
```

哈希值作为 metadata 的一部分存储在 PostgreSQL 的 newsletters 表中。

## 当前实现状态

### 已实现但未激活的功能

1. **内容哈希计算**: ✅ 已实现
   - 每个上传的文档都会计算 SHA-256 哈希值
   - 哈希值会存储在数据库的 metadata 字段中

2. **重复检测函数**: ✅ 已实现
   - `check_duplicate_by_content_hash()` 函数可以检测重复
   - 但在实际上传流程中**未被调用**

3. **哈希存储**: ✅ 已实现
   - 哈希值正确存储在 PostgreSQL 中
   - 可以用于后续的查询和比对

### 缺失的部分

当前的上传流程（`process_and_index` 函数）中**没有调用**重复检测函数，导致：
- 相同的文件可以被重复上传
- 每次上传都会创建新的数据库记录
- MinIO 中会存储重复的文件对象

## 文件上传流程

```
用户上传文件
    ↓
计算 SHA-256 哈希值
    ↓
❌ 未进行重复检测（应该在这里调用 check_duplicate_by_content_hash）
    ↓
上传文件到 MinIO
    ↓
插入记录到 PostgreSQL（包含哈希值）
    ↓
索引到 Elasticsearch
```

## 如何激活去重功能

要完全实现文件去重，需要在 `article_processing_service.py` 的 `process_and_index` 函数中添加重复检测逻辑：

```python
# 在第240行（计算哈希后）添加：
content_hash = hashlib.sha256(file_content).hexdigest()

# 检查是否已存在相同内容的文件
existing_id = await postgresql_service.check_duplicate_by_content_hash(content_hash)
if existing_id:
    # 文件已存在，返回已有记录
    return {
        "success": True,
        "doc_id": existing_id,
        "is_duplicate": True,
        "message": "文件内容已存在，返回已有记录"
    }

# 如果不存在，继续正常的上传流程...
```

## MinIO 层面的处理

MinIO 本身使用 ETag（通常是 MD5 哈希）来标识对象，但：
- ETag 主要用于验证数据完整性
- MinIO 允许相同内容的文件以不同名称存储
- 系统没有基于 ETag 进行去重

## 优化建议

1. **激活重复检测**
   - 在上传流程中调用 `check_duplicate_by_content_hash()`
   - 对于重复文件，可以选择：
     - 直接返回已存在的记录 ID
     - 或更新已有记录的元数据

2. **MinIO 存储优化**
   - 对于重复内容，可以只在 MinIO 中保存一份
   - 使用软链接或引用计数机制管理多个逻辑文件名

3. **性能优化**
   - 为 `metadata->>'content_hash'` 创建索引以加快查询
   - 考虑在 Redis 中缓存哈希值以减少数据库查询

4. **用户体验**
   - 提供选项让用户选择是否覆盖或创建新版本
   - 显示文件重复提示并给出已存在文件的信息

## 数据库索引优化

为了提高重复检测的性能，建议添加索引：

```sql
CREATE INDEX idx_newsletters_content_hash 
ON newsletters ((metadata->>'content_hash'));
```

## 总结

系统已经具备了文件去重的基础设施：
- ✅ SHA-256 哈希计算
- ✅ 哈希值存储
- ✅ 重复检测函数

但核心的重复检测逻辑尚未集成到上传流程中，需要简单的代码修改即可激活完整的去重功能。